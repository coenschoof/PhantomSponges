{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "configuration:\n",
    "models_ver - insert YOLO version's numbers that the UAP will be trained on.\n",
    "\n",
    "epsilon, lambda_1, lambda_2 - attack's parameters. more information can be found in the [paper](https://arxiv.org/abs/2205.13618)\n",
    "\n",
    "BDD_IMG_DIR - a path to the BDD validation set images (or any other wanted dataset)\n",
    "\n",
    "BDD_LAB_DIR - a path to the BDD validation set labels (or any other wanted dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models_vers = [5] # for example: models_vers = [5] or models_vers = [3, 4, 5]\n",
    "epsilon = 70\n",
    "lambda_1 = 1\n",
    "lambda_2 = 10\n",
    "seed = 42\n",
    "patch_size=(640,640)\n",
    "img_size=(640,640)\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "max_labels_per_img = 65\n",
    "BDD_IMG_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val'\n",
    "BDD_LAB_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/labels/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load BDD dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coenschoof/miniconda/envs/phantomsponges/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "from datasets.augmentations1 import train_transform\n",
    "from datasets.split_data_set_combined import SplitDatasetCombined_BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# batch = [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "\n",
    "# unzipped = zip(*batch)\n",
    "\n",
    "# numbers, letters = unzipped\n",
    "\n",
    "# print(numbers)  # Output: (1, 2, 3)\n",
    "# print(letters)  # Output: ('a', 'b', 'c')\n",
    "\n",
    "#dus ipv [(tensor, array, string), (tensor, array, string), (tensor, array, string)] hebben we\n",
    "# train_loader[0] bevat 8 images\n",
    "# train_loader[1] bevat alle labels per voor 8 images\n",
    "# train_loader[2] bevat paths naar 8 images\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def set_random_seed(seed_value, use_cuda=True):\n",
    "    numpy.random.seed(seed_value)  # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu  vars\n",
    "    random.seed(seed_value)  # Python\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # Python hash buildin\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  # needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = SplitDatasetCombined_BDD(\n",
    "            img_dir= BDD_IMG_DIR,\n",
    "            lab_dir= BDD_LAB_DIR,\n",
    "            max_lab=max_labels_per_img,\n",
    "            img_size=img_size,\n",
    "            transform=train_transform,\n",
    "            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader en val_loader \n",
    "\n",
    "#train loader bevat een tuple van 3, elk element van de tuple bevat 8 samples (8 samples maakt 1 batch)\n",
    "#De tuple bestaat uit het volgende:\n",
    "    #[0] = de image met size (3,640,640)\n",
    "    #[1] = de bounding boxes met size (x, 5) (5 bestaat uit class, x, y, x, y)\n",
    "        #lijkt nergens gebruikt te worden\n",
    "    #[2] = de naam van de image (e.g. 'c98258a4-54a47ff2.jpg')  \n",
    "\n",
    "    #HEB IK AL DEZE INFORMATIE OOK NODIG WANNEER IK WIL TESTEN OP PASCAL/MTSD? converten is een pain\n",
    "        #WORDT [2] ERGENS GEBRUIKT? IK WEET DAT [1] NERGENS GEBRUIKT WORDT\n",
    "train_loader, val_loader, test_loader = split_dataset(val_split=0.1,\n",
    "                                                      shuffle_dataset=True,\n",
    "                                                      random_seed=seed,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      ordered=False,\n",
    "                                                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE FOR EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from local_yolos.yolov5.models.experimental import attempt_load\n",
    "from local_yolos.yolov5.utils.general import non_max_suppression\n",
    "from torchvision import transforms\n",
    "from local_yolos.yolov5.utils.metrics import box_iou\n",
    "from local_yolos.yolov5.utils.plots import Annotator\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_img(img_name, change_aspect_ratio = True, output_type = \"torch\", patch_name = 'final_patch.png'):\n",
    "    image_path = \"/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val/\" + img_name\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((640, 640))\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    clean_img = transform(image)\n",
    "\n",
    "    # Load the image using PIL\n",
    "    patch_path =  \"/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges/final_patch.png\"\n",
    "    patch = Image.open(patch_path)\n",
    "    patch = transform(patch)\n",
    "\n",
    "    perturbed_img = torch.clamp(clean_img + patch, 0, 1)\n",
    "\n",
    "    # Assuming you have a torch tensor named torch_image\n",
    "    # You can resize the tensor to (H, W, C) shape using transpose and multiply by 255 (if it's not already in the correct range)\n",
    "    #pil_image = transforms.ToPILImage()(result.squeeze().cpu() * 255)\n",
    "    tensor_min = torch.min(perturbed_img)\n",
    "    tensor_max = torch.max(perturbed_img)\n",
    "    normalized_tensor = (perturbed_img - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Convert the normalized tensor to a PIL image\n",
    "    pil_image = transforms.ToPILImage()(normalized_tensor.squeeze().cpu())\n",
    "    if change_aspect_ratio:\n",
    "        random_scale_w = random.uniform(0.7, 0.9) #1.5, 3\n",
    "        #random_scale_h = random.uniform(1, 2.5)\n",
    "        width = int(640 * random_scale_w)\n",
    "        width = (width // 32) * 32 #ensures that the new width is a multiple of 32, otherwise the model does not work\n",
    "        height = 640 #height = int(640 * random_scale_h)\n",
    "        height = (height // 32) * 32\n",
    "        pil_image = pil_image.resize((width, height))\n",
    "\n",
    "    if output_type == \"torch\":\n",
    "        perturbed_image = transform(pil_image)\n",
    "    elif output_type == \"numpy\":\n",
    "        perturbed_image = np.array(pil_image)\n",
    "    elif output == \"pil\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Unsupported output type provided!\")\n",
    "\n",
    "    return(perturbed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:07<00:00, 67.71it/s]\n"
     ]
    }
   ],
   "source": [
    "image_names = []\n",
    "for image_np, _, image_name in tqdm(test_loader):\n",
    "    image_names.append(image_name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New loss term experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tiny batch containing 3 images; 2 random images from the test set, and a fully white image\n",
    "\n",
    "zeros_tensor = torch.zeros(1, 3, 640, 640)\n",
    "#zero_output = model(zeros_tensor)[0]\n",
    "\n",
    "random_index = random.randrange(len(image_names))\n",
    "other_image = perturb_img(image_names[random_index], change_aspect_ratio = False, output_type = \"torch\").unsqueeze(0)\n",
    "random_index = random.randrange(len(image_names))\n",
    "other_image_2 = perturb_img(image_names[random_index], change_aspect_ratio = False, output_type = \"torch\").unsqueeze(0)\n",
    "pil_image = transforms.ToPILImage()(other_image.squeeze())\n",
    "#display(pil_image)\n",
    "\n",
    "tiny_batch = torch.cat((other_image, other_image_2, zeros_tensor))\n",
    "#tiny_batch_output = model(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kzjlsdfhalsdkjfhalskdjhfs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classify': {'model': ultralytics.nn.tasks.ClassificationModel,\n",
       "  'trainer': ultralytics.models.yolo.classify.train.ClassificationTrainer,\n",
       "  'validator': ultralytics.models.yolo.classify.val.ClassificationValidator,\n",
       "  'predictor': ultralytics.models.yolo.classify.predict.ClassificationPredictor},\n",
       " 'detect': {'model': ultralytics.nn.tasks.DetectionModel,\n",
       "  'trainer': ultralytics.models.yolo.detect.train.DetectionTrainer,\n",
       "  'validator': ultralytics.models.yolo.detect.val.DetectionValidator,\n",
       "  'predictor': ultralytics.models.yolo.detect.predict.DetectionPredictor},\n",
       " 'segment': {'model': ultralytics.nn.tasks.SegmentationModel,\n",
       "  'trainer': ultralytics.models.yolo.segment.train.SegmentationTrainer,\n",
       "  'validator': ultralytics.models.yolo.segment.val.SegmentationValidator,\n",
       "  'predictor': ultralytics.models.yolo.segment.predict.SegmentationPredictor},\n",
       " 'pose': {'model': ultralytics.nn.tasks.PoseModel,\n",
       "  'trainer': ultralytics.models.yolo.pose.train.PoseTrainer,\n",
       "  'validator': ultralytics.models.yolo.pose.val.PoseValidator,\n",
       "  'predictor': ultralytics.models.yolo.pose.predict.PosePredictor}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.task_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 cars, 1 bus, 2 trucks, 1: 640x640 11 cars, 1 truck, 2: 640x640 (no detections), 884.9ms\n",
      "Speed: 1.0ms preprocess, 295.0ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "results = model(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: tensor([[[[0.3922, 0.4039, 0.3882,  ..., 0.9255, 0.9255, 0.9294],\n",
       "           [0.3922, 0.3922, 0.3882,  ..., 0.9176, 0.9255, 0.9451],\n",
       "           [0.3882, 0.3922, 0.4039,  ..., 0.9255, 0.9373, 0.9373],\n",
       "           ...,\n",
       "           [0.1451, 0.1216, 0.1922,  ..., 0.1255, 0.1176, 0.1765],\n",
       "           [0.1216, 0.1137, 0.2235,  ..., 0.1098, 0.1098, 0.1098],\n",
       "           [0.1529, 0.2157, 0.1529,  ..., 0.2039, 0.1608, 0.1333]],\n",
       " \n",
       "          [[0.6275, 0.6314, 0.6275,  ..., 1.0000, 0.9922, 0.9843],\n",
       "           [0.6314, 0.6275, 0.6275,  ..., 0.9961, 0.9882, 0.9961],\n",
       "           [0.6314, 0.6275, 0.6392,  ..., 1.0000, 1.0000, 0.9961],\n",
       "           ...,\n",
       "           [0.2275, 0.1608, 0.1451,  ..., 0.1569, 0.1451, 0.1451],\n",
       "           [0.1804, 0.1647, 0.1922,  ..., 0.1412, 0.1412, 0.1412],\n",
       "           [0.2000, 0.2196, 0.1725,  ..., 0.1412, 0.1412, 0.1412]],\n",
       " \n",
       "          [[0.8510, 0.8549, 0.8510,  ..., 0.9922, 0.9961, 0.9922],\n",
       "           [0.8431, 0.8471, 0.8588,  ..., 0.9882, 0.9922, 0.9922],\n",
       "           [0.8471, 0.8588, 0.8588,  ..., 0.9882, 0.9922, 1.0000],\n",
       "           ...,\n",
       "           [0.1961, 0.1804, 0.1686,  ..., 0.2667, 0.2431, 0.2078],\n",
       "           [0.1725, 0.1686, 0.1647,  ..., 0.1765, 0.1765, 0.1765],\n",
       "           [0.1882, 0.2118, 0.1647,  ..., 0.2000, 0.1765, 0.1765]]],\n",
       " \n",
       " \n",
       "         [[[0.2510, 0.2000, 0.1843,  ..., 0.6549, 0.6118, 0.6863],\n",
       "           [0.2431, 0.1922, 0.1843,  ..., 0.6471, 0.6118, 0.6980],\n",
       "           [0.2275, 0.2078, 0.2039,  ..., 0.6549, 0.6235, 0.6902],\n",
       "           ...,\n",
       "           [0.1529, 0.1333, 0.2157,  ..., 0.2627, 0.2549, 0.3137],\n",
       "           [0.1451, 0.1412, 0.2627,  ..., 0.2549, 0.2549, 0.2549],\n",
       "           [0.1725, 0.2431, 0.1882,  ..., 0.3490, 0.3059, 0.2784]],\n",
       " \n",
       "          [[0.2588, 0.2000, 0.1961,  ..., 0.6235, 0.5804, 0.6510],\n",
       "           [0.2549, 0.2000, 0.1961,  ..., 0.6196, 0.5765, 0.6588],\n",
       "           [0.2431, 0.2157, 0.2118,  ..., 0.6157, 0.5882, 0.6549],\n",
       "           ...,\n",
       "           [0.2235, 0.1608, 0.1569,  ..., 0.3098, 0.2980, 0.2980],\n",
       "           [0.1922, 0.1804, 0.2196,  ..., 0.3020, 0.3020, 0.3020],\n",
       "           [0.2078, 0.2353, 0.1961,  ..., 0.3020, 0.3020, 0.3020]],\n",
       " \n",
       "          [[0.3020, 0.2431, 0.2392,  ..., 0.5490, 0.5216, 0.5961],\n",
       "           [0.2863, 0.2392, 0.2471,  ..., 0.5490, 0.5216, 0.6000],\n",
       "           [0.2784, 0.2667, 0.2510,  ..., 0.5529, 0.5255, 0.6118],\n",
       "           ...,\n",
       "           [0.2353, 0.2235, 0.2235,  ..., 0.4863, 0.4627, 0.4275],\n",
       "           [0.2275, 0.2275, 0.2353,  ..., 0.4078, 0.4078, 0.4078],\n",
       "           [0.2392, 0.2706, 0.2314,  ..., 0.4314, 0.4078, 0.4078]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n",
       " orig_shape: torch.Size([3, 3])\n",
       " path: 'image0.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 0.0067551930745442705, 'inference': 296.45808537801105, 'postprocess': 1.3289451599121094},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: tensor([[[[0.3922, 0.4039, 0.3882,  ..., 0.9255, 0.9255, 0.9294],\n",
       "           [0.3922, 0.3922, 0.3882,  ..., 0.9176, 0.9255, 0.9451],\n",
       "           [0.3882, 0.3922, 0.4039,  ..., 0.9255, 0.9373, 0.9373],\n",
       "           ...,\n",
       "           [0.1451, 0.1216, 0.1922,  ..., 0.1255, 0.1176, 0.1765],\n",
       "           [0.1216, 0.1137, 0.2235,  ..., 0.1098, 0.1098, 0.1098],\n",
       "           [0.1529, 0.2157, 0.1529,  ..., 0.2039, 0.1608, 0.1333]],\n",
       " \n",
       "          [[0.6275, 0.6314, 0.6275,  ..., 1.0000, 0.9922, 0.9843],\n",
       "           [0.6314, 0.6275, 0.6275,  ..., 0.9961, 0.9882, 0.9961],\n",
       "           [0.6314, 0.6275, 0.6392,  ..., 1.0000, 1.0000, 0.9961],\n",
       "           ...,\n",
       "           [0.2275, 0.1608, 0.1451,  ..., 0.1569, 0.1451, 0.1451],\n",
       "           [0.1804, 0.1647, 0.1922,  ..., 0.1412, 0.1412, 0.1412],\n",
       "           [0.2000, 0.2196, 0.1725,  ..., 0.1412, 0.1412, 0.1412]],\n",
       " \n",
       "          [[0.8510, 0.8549, 0.8510,  ..., 0.9922, 0.9961, 0.9922],\n",
       "           [0.8431, 0.8471, 0.8588,  ..., 0.9882, 0.9922, 0.9922],\n",
       "           [0.8471, 0.8588, 0.8588,  ..., 0.9882, 0.9922, 1.0000],\n",
       "           ...,\n",
       "           [0.1961, 0.1804, 0.1686,  ..., 0.2667, 0.2431, 0.2078],\n",
       "           [0.1725, 0.1686, 0.1647,  ..., 0.1765, 0.1765, 0.1765],\n",
       "           [0.1882, 0.2118, 0.1647,  ..., 0.2000, 0.1765, 0.1765]]],\n",
       " \n",
       " \n",
       "         [[[0.2510, 0.2000, 0.1843,  ..., 0.6549, 0.6118, 0.6863],\n",
       "           [0.2431, 0.1922, 0.1843,  ..., 0.6471, 0.6118, 0.6980],\n",
       "           [0.2275, 0.2078, 0.2039,  ..., 0.6549, 0.6235, 0.6902],\n",
       "           ...,\n",
       "           [0.1529, 0.1333, 0.2157,  ..., 0.2627, 0.2549, 0.3137],\n",
       "           [0.1451, 0.1412, 0.2627,  ..., 0.2549, 0.2549, 0.2549],\n",
       "           [0.1725, 0.2431, 0.1882,  ..., 0.3490, 0.3059, 0.2784]],\n",
       " \n",
       "          [[0.2588, 0.2000, 0.1961,  ..., 0.6235, 0.5804, 0.6510],\n",
       "           [0.2549, 0.2000, 0.1961,  ..., 0.6196, 0.5765, 0.6588],\n",
       "           [0.2431, 0.2157, 0.2118,  ..., 0.6157, 0.5882, 0.6549],\n",
       "           ...,\n",
       "           [0.2235, 0.1608, 0.1569,  ..., 0.3098, 0.2980, 0.2980],\n",
       "           [0.1922, 0.1804, 0.2196,  ..., 0.3020, 0.3020, 0.3020],\n",
       "           [0.2078, 0.2353, 0.1961,  ..., 0.3020, 0.3020, 0.3020]],\n",
       " \n",
       "          [[0.3020, 0.2431, 0.2392,  ..., 0.5490, 0.5216, 0.5961],\n",
       "           [0.2863, 0.2392, 0.2471,  ..., 0.5490, 0.5216, 0.6000],\n",
       "           [0.2784, 0.2667, 0.2510,  ..., 0.5529, 0.5255, 0.6118],\n",
       "           ...,\n",
       "           [0.2353, 0.2235, 0.2235,  ..., 0.4863, 0.4627, 0.4275],\n",
       "           [0.2275, 0.2275, 0.2353,  ..., 0.4078, 0.4078, 0.4078],\n",
       "           [0.2392, 0.2706, 0.2314,  ..., 0.4314, 0.4078, 0.4078]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n",
       " orig_shape: torch.Size([3, 3])\n",
       " path: 'image1.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 0.0067551930745442705, 'inference': 296.45808537801105, 'postprocess': 1.3289451599121094},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: tensor([[[[0.3922, 0.4039, 0.3882,  ..., 0.9255, 0.9255, 0.9294],\n",
       "           [0.3922, 0.3922, 0.3882,  ..., 0.9176, 0.9255, 0.9451],\n",
       "           [0.3882, 0.3922, 0.4039,  ..., 0.9255, 0.9373, 0.9373],\n",
       "           ...,\n",
       "           [0.1451, 0.1216, 0.1922,  ..., 0.1255, 0.1176, 0.1765],\n",
       "           [0.1216, 0.1137, 0.2235,  ..., 0.1098, 0.1098, 0.1098],\n",
       "           [0.1529, 0.2157, 0.1529,  ..., 0.2039, 0.1608, 0.1333]],\n",
       " \n",
       "          [[0.6275, 0.6314, 0.6275,  ..., 1.0000, 0.9922, 0.9843],\n",
       "           [0.6314, 0.6275, 0.6275,  ..., 0.9961, 0.9882, 0.9961],\n",
       "           [0.6314, 0.6275, 0.6392,  ..., 1.0000, 1.0000, 0.9961],\n",
       "           ...,\n",
       "           [0.2275, 0.1608, 0.1451,  ..., 0.1569, 0.1451, 0.1451],\n",
       "           [0.1804, 0.1647, 0.1922,  ..., 0.1412, 0.1412, 0.1412],\n",
       "           [0.2000, 0.2196, 0.1725,  ..., 0.1412, 0.1412, 0.1412]],\n",
       " \n",
       "          [[0.8510, 0.8549, 0.8510,  ..., 0.9922, 0.9961, 0.9922],\n",
       "           [0.8431, 0.8471, 0.8588,  ..., 0.9882, 0.9922, 0.9922],\n",
       "           [0.8471, 0.8588, 0.8588,  ..., 0.9882, 0.9922, 1.0000],\n",
       "           ...,\n",
       "           [0.1961, 0.1804, 0.1686,  ..., 0.2667, 0.2431, 0.2078],\n",
       "           [0.1725, 0.1686, 0.1647,  ..., 0.1765, 0.1765, 0.1765],\n",
       "           [0.1882, 0.2118, 0.1647,  ..., 0.2000, 0.1765, 0.1765]]],\n",
       " \n",
       " \n",
       "         [[[0.2510, 0.2000, 0.1843,  ..., 0.6549, 0.6118, 0.6863],\n",
       "           [0.2431, 0.1922, 0.1843,  ..., 0.6471, 0.6118, 0.6980],\n",
       "           [0.2275, 0.2078, 0.2039,  ..., 0.6549, 0.6235, 0.6902],\n",
       "           ...,\n",
       "           [0.1529, 0.1333, 0.2157,  ..., 0.2627, 0.2549, 0.3137],\n",
       "           [0.1451, 0.1412, 0.2627,  ..., 0.2549, 0.2549, 0.2549],\n",
       "           [0.1725, 0.2431, 0.1882,  ..., 0.3490, 0.3059, 0.2784]],\n",
       " \n",
       "          [[0.2588, 0.2000, 0.1961,  ..., 0.6235, 0.5804, 0.6510],\n",
       "           [0.2549, 0.2000, 0.1961,  ..., 0.6196, 0.5765, 0.6588],\n",
       "           [0.2431, 0.2157, 0.2118,  ..., 0.6157, 0.5882, 0.6549],\n",
       "           ...,\n",
       "           [0.2235, 0.1608, 0.1569,  ..., 0.3098, 0.2980, 0.2980],\n",
       "           [0.1922, 0.1804, 0.2196,  ..., 0.3020, 0.3020, 0.3020],\n",
       "           [0.2078, 0.2353, 0.1961,  ..., 0.3020, 0.3020, 0.3020]],\n",
       " \n",
       "          [[0.3020, 0.2431, 0.2392,  ..., 0.5490, 0.5216, 0.5961],\n",
       "           [0.2863, 0.2392, 0.2471,  ..., 0.5490, 0.5216, 0.6000],\n",
       "           [0.2784, 0.2667, 0.2510,  ..., 0.5529, 0.5255, 0.6118],\n",
       "           ...,\n",
       "           [0.2353, 0.2235, 0.2235,  ..., 0.4863, 0.4627, 0.4275],\n",
       "           [0.2275, 0.2275, 0.2353,  ..., 0.4078, 0.4078, 0.4078],\n",
       "           [0.2392, 0.2706, 0.2314,  ..., 0.4314, 0.4078, 0.4078]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n",
       " orig_shape: torch.Size([3, 3])\n",
       " path: 'image2.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 0.0067551930745442705, 'inference': 296.45808537801105, 'postprocess': 1.3289451599121094}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
