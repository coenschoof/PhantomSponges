{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "configuration:\n",
    "models_ver - insert YOLO version's numbers that the UAP will be trained on.\n",
    "\n",
    "epsilon, lambda_1, lambda_2 - attack's parameters. more information can be found in the [paper](https://arxiv.org/abs/2205.13618)\n",
    "\n",
    "BDD_IMG_DIR - a path to the BDD validation set images (or any other wanted dataset)\n",
    "\n",
    "BDD_LAB_DIR - a path to the BDD validation set labels (or any other wanted dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models_vers = [5] # for example: models_vers = [5] or models_vers = [3, 4, 5]\n",
    "epsilon = 70\n",
    "lambda_1 = 1\n",
    "lambda_2 = 10\n",
    "seed = 42\n",
    "patch_size=(640,640)\n",
    "img_size=(640,640)\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "max_labels_per_img = 65\n",
    "BDD_IMG_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val'\n",
    "BDD_LAB_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/labels/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load BDD dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coenschoof/miniconda/envs/phantomsponges/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "from datasets.augmentations1 import train_transform\n",
    "from datasets.split_data_set_combined import SplitDatasetCombined_BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# batch = [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "\n",
    "# unzipped = zip(*batch)\n",
    "\n",
    "# numbers, letters = unzipped\n",
    "\n",
    "# print(numbers)  # Output: (1, 2, 3)\n",
    "# print(letters)  # Output: ('a', 'b', 'c')\n",
    "\n",
    "#dus ipv [(tensor, array, string), (tensor, array, string), (tensor, array, string)] hebben we\n",
    "# train_loader[0] bevat 8 images\n",
    "# train_loader[1] bevat alle labels per voor 8 images\n",
    "# train_loader[2] bevat paths naar 8 images\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def set_random_seed(seed_value, use_cuda=True):\n",
    "    numpy.random.seed(seed_value)  # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu  vars\n",
    "    random.seed(seed_value)  # Python\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # Python hash buildin\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  # needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = SplitDatasetCombined_BDD(\n",
    "            img_dir= BDD_IMG_DIR,\n",
    "            lab_dir= BDD_LAB_DIR,\n",
    "            max_lab=max_labels_per_img,\n",
    "            img_size=img_size,\n",
    "            transform=train_transform,\n",
    "            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader en val_loader \n",
    "\n",
    "#train loader bevat een tuple van 3, elk element van de tuple bevat 8 samples (8 samples maakt 1 batch)\n",
    "#De tuple bestaat uit het volgende:\n",
    "    #[0] = de image met size (3,640,640)\n",
    "    #[1] = de bounding boxes met size (x, 5) (5 bestaat uit class, x, y, x, y)\n",
    "        #lijkt nergens gebruikt te worden\n",
    "    #[2] = de naam van de image (e.g. 'c98258a4-54a47ff2.jpg')  \n",
    "\n",
    "    #HEB IK AL DEZE INFORMATIE OOK NODIG WANNEER IK WIL TESTEN OP PASCAL/MTSD? converten is een pain\n",
    "        #WORDT [2] ERGENS GEBRUIKT? IK WEET DAT [1] NERGENS GEBRUIKT WORDT\n",
    "train_loader, val_loader, test_loader = split_dataset(val_split=0.1,\n",
    "                                                      shuffle_dataset=True,\n",
    "                                                      random_seed=seed,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      ordered=False,\n",
    "                                                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE FOR EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from local_yolos.yolov5.models.experimental import attempt_load\n",
    "from local_yolos.yolov5.utils.general import non_max_suppression\n",
    "from torchvision import transforms\n",
    "from local_yolos.yolov5.utils.metrics import box_iou\n",
    "from local_yolos.yolov5.utils.plots import Annotator\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_img(img_name, change_aspect_ratio = True, output_type = \"torch\", patch_name = 'final_patch.png'):\n",
    "    image_path = \"/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val/\" + img_name\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((640, 640))\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    clean_img = transform(image)\n",
    "\n",
    "    # Load the image using PIL\n",
    "    patch_path =  \"/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges/final_patch.png\"\n",
    "    patch = Image.open(patch_path)\n",
    "    patch = transform(patch)\n",
    "\n",
    "    perturbed_img = torch.clamp(clean_img + patch, 0, 1)\n",
    "\n",
    "    # Assuming you have a torch tensor named torch_image\n",
    "    # You can resize the tensor to (H, W, C) shape using transpose and multiply by 255 (if it's not already in the correct range)\n",
    "    #pil_image = transforms.ToPILImage()(result.squeeze().cpu() * 255)\n",
    "    tensor_min = torch.min(perturbed_img)\n",
    "    tensor_max = torch.max(perturbed_img)\n",
    "    normalized_tensor = (perturbed_img - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Convert the normalized tensor to a PIL image\n",
    "    pil_image = transforms.ToPILImage()(normalized_tensor.squeeze().cpu())\n",
    "    if change_aspect_ratio:\n",
    "        random_scale_w = random.uniform(0.7, 0.9) #1.5, 3\n",
    "        #random_scale_h = random.uniform(1, 2.5)\n",
    "        width = int(640 * random_scale_w)\n",
    "        width = (width // 32) * 32 #ensures that the new width is a multiple of 32, otherwise the model does not work\n",
    "        height = 640 #height = int(640 * random_scale_h)\n",
    "        height = (height // 32) * 32\n",
    "        pil_image = pil_image.resize((width, height))\n",
    "\n",
    "    if output_type == \"torch\":\n",
    "        perturbed_image = transform(pil_image)\n",
    "    elif output_type == \"numpy\":\n",
    "        perturbed_image = np.array(pil_image)\n",
    "    elif output == \"pil\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Unsupported output type provided!\")\n",
    "\n",
    "    return(perturbed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:08<00:00, 60.89it/s]\n"
     ]
    }
   ],
   "source": [
    "image_names = []\n",
    "for image_np, _, image_name in tqdm(test_loader):\n",
    "    image_names.append(image_name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New loss term experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tiny batch containing 3 images; 2 random images from the test set, and a fully white image\n",
    "\n",
    "zeros_tensor = torch.zeros(1, 3, 640, 640)\n",
    "#zero_output = model(zeros_tensor)[0]\n",
    "\n",
    "random_index = random.randrange(len(image_names))\n",
    "other_image = perturb_img(image_names[random_index], change_aspect_ratio = False, output_type = \"torch\").unsqueeze(0)\n",
    "random_index = random.randrange(len(image_names))\n",
    "other_image_2 = perturb_img(image_names[random_index], change_aspect_ratio = False, output_type = \"torch\").unsqueeze(0)\n",
    "pil_image = transforms.ToPILImage()(other_image.squeeze())\n",
    "#display(pil_image)\n",
    "\n",
    "tiny_batch = torch.cat((other_image, other_image_2, zeros_tensor))\n",
    "#tiny_batch_output = model(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges/local_yolos/yolov8\n"
     ]
    }
   ],
   "source": [
    "cd local_yolos/yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges/local_yolos/yolov8'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_yolos.yolov8.ultralytics.models.yolo.model import YOLO\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "import traceback\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 cars, 1 truck, 1 tv, 1: 640x640 2 persons, 1 car, 2 traffic lights, 2: 640x640 (no detections), 881.2ms\n",
      "Speed: 0.0ms preprocess, 293.7ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "results = model(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_yolos.yolov8.ultralytics.utils import ASSETS\n",
    "from local_yolos.yolov8.ultralytics.models.yolo.detect import DetectionPredictor\n",
    "from local_yolos.yolov8.ultralytics.engine.results import Results\n",
    "from local_yolos.yolov8.ultralytics.utils import ops\n",
    "\n",
    "class CustomPredictor(DetectionPredictor):\n",
    "    # def __call__(self, source=None, model=None, stream=False, *args, **kwargs):\n",
    "    #     \"\"\"Performs inference on an image or stream.\"\"\"\n",
    "    #     self.stream = stream\n",
    "    #     if stream:\n",
    "    #         return self.stream_inference(source, model, *args, **kwargs)\n",
    "    #     else:\n",
    "    #         return list(self.stream_inference(source, model, *args, **kwargs))  \n",
    "        \n",
    "    def postprocess(self, preds, img, orig_imgs):\n",
    "        # print(len(preds))\n",
    "        # print(preds[0].shape)\n",
    "        \"\"\"Post-processes predictions for an image and returns them.\"\"\"\n",
    "        preds = ops.non_max_suppression(prediction = preds,\n",
    "                                        conf_thres = 0.25,\n",
    "                                        iou_thres = 0.45,\n",
    "                                        max_det=300)\n",
    "        \n",
    "        print(len(preds))\n",
    "        print(preds[0])\n",
    "        #return preds\n",
    "        \n",
    "        if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list\n",
    "            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n",
    "\n",
    "        results = []\n",
    "        for i, pred in enumerate(preds):\n",
    "            orig_img = orig_imgs[i]\n",
    "            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\n",
    "            img_path = self.batch[0][i]\n",
    "            results.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.171 🚀 Python-3.9.17 torch-1.9.1 CPU (Apple M1)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "Results saved to \u001b[1mruns/detect/train21\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([[3.0272e+00, 1.6750e-02, 1.9016e+02, 1.2799e+02, 6.3463e-01, 6.2000e+01],\n",
      "        [3.0615e+02, 1.9380e+02, 3.7741e+02, 2.7963e+02, 5.1286e-01, 2.0000e+00],\n",
      "        [3.8956e+02, 1.5574e+02, 6.3824e+02, 3.8544e+02, 3.6889e-01, 7.0000e+00],\n",
      "        [2.5612e+02, 2.1958e+02, 2.8896e+02, 2.3976e+02, 2.6722e-01, 2.0000e+00]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[local_yolos.yolov8.ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: local_yolos.yolov8.ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: array([[[16, 15, 15],\n",
       "         [17, 14, 14],\n",
       "         [ 9,  9,  9],\n",
       "         ...,\n",
       "         [15, 15, 14],\n",
       "         [13, 14, 14],\n",
       "         [13, 13, 13]],\n",
       " \n",
       "        [[16, 16, 13],\n",
       "         [14, 13, 12],\n",
       "         [10, 10, 12],\n",
       "         ...,\n",
       "         [14, 15, 14],\n",
       "         [14, 14, 14],\n",
       "         [18, 17, 14]],\n",
       " \n",
       "        [[15, 16, 14],\n",
       "         [15, 14, 16],\n",
       "         [15, 14, 13],\n",
       "         ...,\n",
       "         [16, 16, 14],\n",
       "         [17, 18, 14],\n",
       "         [16, 17, 16]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 8, 20,  9],\n",
       "         [ 4,  5,  7],\n",
       "         [27,  6,  9],\n",
       "         ...,\n",
       "         [16, 14, 31],\n",
       "         [14, 11, 25],\n",
       "         [29, 11, 16]],\n",
       " \n",
       "        [[ 7, 13,  8],\n",
       "         [ 7, 11,  9],\n",
       "         [37, 20, 10],\n",
       "         ...,\n",
       "         [12, 10,  8],\n",
       "         [12, 10,  8],\n",
       "         [12, 10,  8]],\n",
       " \n",
       "        [[18, 21, 15],\n",
       "         [34, 26, 21],\n",
       "         [18, 14,  9],\n",
       "         ...,\n",
       "         [37, 11, 15],\n",
       "         [26, 11,  9],\n",
       "         [19, 11,  9]]], dtype=uint8)\n",
       " orig_shape: (640, 640)\n",
       " path: 'image0.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs/detect/train21'\n",
       " speed: {'preprocess': 0.00667572021484375, 'inference': 291.66102409362793, 'postprocess': 3.7113825480143228},\n",
       " local_yolos.yolov8.ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: local_yolos.yolov8.ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: array([[[ 2,  1,  3],\n",
       "         [ 7,  4,  6],\n",
       "         [ 4,  4,  6],\n",
       "         ...,\n",
       "         [12, 12, 15],\n",
       "         [12, 13, 17],\n",
       "         [15, 15, 19]],\n",
       " \n",
       "        [[ 4,  4,  3],\n",
       "         [ 5,  4,  5],\n",
       "         [ 5,  5,  9],\n",
       "         ...,\n",
       "         [10, 11, 14],\n",
       "         [12, 12, 16],\n",
       "         [19, 18, 19]],\n",
       " \n",
       "        [[ 4,  5,  5],\n",
       "         [ 6,  5,  9],\n",
       "         [10,  9, 10],\n",
       "         ...,\n",
       "         [12, 12, 14],\n",
       "         [15, 16, 16],\n",
       "         [17, 18, 21]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[13, 25, 12],\n",
       "         [ 8,  9,  9],\n",
       "         [29,  8,  9],\n",
       "         ...,\n",
       "         [13, 11, 28],\n",
       "         [11,  8, 22],\n",
       "         [26,  8, 13]],\n",
       " \n",
       "        [[ 9, 15,  8],\n",
       "         [ 8, 12,  8],\n",
       "         [39, 22, 10],\n",
       "         ...,\n",
       "         [ 9,  7,  5],\n",
       "         [ 9,  7,  5],\n",
       "         [ 9,  7,  5]],\n",
       " \n",
       "        [[16, 19, 11],\n",
       "         [34, 26, 19],\n",
       "         [20, 16,  9],\n",
       "         ...,\n",
       "         [33,  7, 11],\n",
       "         [22,  7,  5],\n",
       "         [15,  7,  5]]], dtype=uint8)\n",
       " orig_shape: (640, 640)\n",
       " path: 'image1.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs/detect/train21'\n",
       " speed: {'preprocess': 0.00667572021484375, 'inference': 291.66102409362793, 'postprocess': 3.7113825480143228},\n",
       " local_yolos.yolov8.ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: local_yolos.yolov8.ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8)\n",
       " orig_shape: (640, 640)\n",
       " path: 'image2.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs/detect/train21'\n",
       " speed: {'preprocess': 0.00667572021484375, 'inference': 291.66102409362793, 'postprocess': 3.7113825480143228}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ONDERSTAANDE OUTPUT MOET NAAR DEZE SHAPE WORDEN OMGEZET\n",
    "\n",
    "#output_patch.size() = torch.Size([8, 25200, 85])\n",
    "    #8 plaatjes per batch\n",
    "    #25200 bounding boxes per plaatje\n",
    "    #85 = (x, y, width, height), object confidence score, and class probabilities. (dus 80 class probs)\n",
    "#print(output_patch.size())\n",
    "\n",
    "\n",
    "args = dict(model='yolov8n.pt', source=tiny_batch, verbose=False)    \n",
    "predictor = CustomPredictor(overrides = args)\n",
    "predictor()\n",
    "\n",
    "# predictor = DetectionPredictor(overrides=args)\n",
    "# predictor.predict_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges/local_yolos/yolov8'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.task_map[self.task][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ultralytics.models.yolo.detect.predict.DetectionPredictor at 0x7fc209159be0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.task_map['detect']['predictor']()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
