{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coenschoof/miniconda/envs/phantomsponges/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from local_yolos.yolov5.models.experimental import attempt_load\n",
    "from local_yolos.yolov5.utils.general import non_max_suppression\n",
    "from torchvision import transforms\n",
    "from local_yolos.yolov5.utils.metrics import box_iou\n",
    "from local_yolos.yolov5.utils.plots import Annotator\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "from datasets.augmentations1 import train_transform\n",
    "from datasets.split_data_set_combined import SplitDatasetCombined_BDD\n",
    "\n",
    "from local_yolos.yolov8.ultralytics.nn.tasks import attempt_load_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script\", flush=True)\n",
    "models_vers = [5] # for example: models_vers = [5] or models_vers = [3, 4, 5]\n",
    "epsilon = 30 \n",
    "lambda_1 = 0.6 #1\n",
    "lambda_2 = 10 #10\n",
    "seed = 42\n",
    "patch_size=(640,640)\n",
    "img_size=(640,640)\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "max_labels_per_img = 65\n",
    "BDD_IMG_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val'\n",
    "BDD_LAB_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/labels/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def set_random_seed(seed_value, use_cuda=True):\n",
    "    np.random.seed(seed_value)  # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu  vars\n",
    "    random.seed(seed_value)  # Python\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # Python hash buildin\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  # needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = SplitDatasetCombined_BDD(\n",
    "            img_dir= BDD_IMG_DIR,\n",
    "            lab_dir= BDD_LAB_DIR,\n",
    "            max_lab=max_labels_per_img,\n",
    "            img_size=img_size,\n",
    "            transform=train_transform,\n",
    "            collate_fn=collate_fn)\n",
    "\n",
    "train_loader, val_loader, test_loader = split_dataset(val_split=0.1,\n",
    "                                                      shuffle_dataset=True,\n",
    "                                                      random_seed=seed,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      ordered=False,\n",
    "                                                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PRO TIP ðŸ’¡ Replace 'model=local_yolos/yolov5/weights/yolov5s.pt' with new 'model=local_yolos/yolov5/weights/yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5su.pt to 'local_yolos/yolov5/weights/yolov5su.pt'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.7M/17.7M [00:00<00:00, 32.2MB/s]\n",
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "YOLOv5s summary (fused): 193 layers, 9142496 parameters, 0 gradients, 24.0 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# Load the YOLOv5 model\n",
    "#model = attempt_load(weights='/ceph/csedu-scratch/project/cschoof/PhantomSponges/local_yolos/yolov5/weights/yolov5s.pt').eval()\n",
    "#model = attempt_load_weights('yolov8n.pt', device=torch.device('cuda'), inplace=True, fuse=True).to('cuda')\n",
    "from attacks_tools.yolov8_wrapper import CustomPredictor\n",
    "#model = CustomPredictor(overrides = dict(model='yolov8n.pt'))\n",
    "model = CustomPredictor(weights='local_yolos/yolov5/weights/yolov5s.pt').to('cpu')\n",
    "\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# if name == 'yolov8':\n",
    "#     from local_yolos.yolov8.ultralytics.nn.tasks import attempt_load_weights\n",
    "#     from attacks_tools.yolov8_wrapper import CustomDetectionModelWrapper\n",
    "#     self.model_size = 'yolov8' + self.model_size + '.pt'\n",
    "#     model = attempt_load_weights(weights='yolov8s.pt', device=device, inplace=True, fuse=True)\n",
    "#     model = CustomDetectionModelWrapper(model)\n",
    "#     #model = CustomPredictor(weights='local_yolos/yolov8/weights/yolov8s.pt').to(device)\n",
    "#     #print(\"Model Device:\", next(model.parameters()).device)\n",
    "# if name == 'yolov5':\n",
    "#     # taken from https://github.com/ultralytics/yolov5\n",
    "#     from local_yolos.yolov5.models.experimental import attempt_load\n",
    "#     model = attempt_load('local_yolos/yolov5/weights/yolov5s.pt', device).eval()\n",
    "# elif name == 'yolov4':\n",
    "#     # taken from https://github.com/WongKinYiu/PyTorch_YOLOv4\n",
    "#     from local_yolos.yolov4.models.models import Darknet, load_darknet_weights\n",
    "#     model = Darknet('local_yolos/yolov4/cfg/yolov4.cfg', img_size=640).to(device).eval()\n",
    "#     load_darknet_weights(model, 'local_yolos/yolov4/weights/yolov4.weights')\n",
    "# elif name == 'yolov3':\n",
    "#     # taken from https://github.com/ultralytics/yolov3\n",
    "#     from local_yolos.yolov3 import hubconf\n",
    "#     model = hubconf.yolov3(pretrained=True, autoshape=False, device=device)\n",
    "# return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_img(img_name, change_aspect_ratio = True, output_type = \"torch\"):\n",
    "    image_path = \"/ceph/csedu-scratch/project/cschoof/bdd_in_YOLOV5_val/images/val/\" + img_name\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((640, 640))\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    clean_img = transform(image)\n",
    "\n",
    "    # Load the image using PIL\n",
    "    #patch_path =  \"/home/cschoof/experiments/yolov_5_epsilon=30_lambda1=0.6_lambda2=10/final_results/final_patch.png\"\n",
    "    patch_path =  \"/home/cschoof/experiments/yolov_5_eps=30_lmbd1=0.6_lmbd2=10_newLoss=True_modelSize=s_dataset=bdd/final_results/final_patch.png\"\n",
    "    patch = Image.open(patch_path)\n",
    "    patch = transform(patch)\n",
    "\n",
    "    perturbed_img = torch.clamp(clean_img + patch, 0, 1)\n",
    "\n",
    "    # Assuming you have a torch tensor named torch_image\n",
    "    # You can resize the tensor to (H, W, C) shape using transpose and multiply by 255 (if it's not already in the correct range)\n",
    "    #pil_image = transforms.ToPILImage()(result.squeeze().cpu() * 255)\n",
    "    tensor_min = torch.min(perturbed_img)\n",
    "    tensor_max = torch.max(perturbed_img)\n",
    "    normalized_tensor = (perturbed_img - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Convert the normalized tensor to a PIL image\n",
    "    pil_image = transforms.ToPILImage()(normalized_tensor.squeeze().cpu())\n",
    "    if change_aspect_ratio:\n",
    "        random_scale_w = random.uniform(0.8, 0.95)\n",
    "        #random_scale_h = random.uniform(1, 2.5)\n",
    "        width = int(640 * random_scale_w)\n",
    "        width = (width // 32) * 32 #ensures that the new width is a multiple of 32, otherwise the model does not work\n",
    "        height = 640 #height = int(640 * random_scale_h)\n",
    "        height = (height // 32) * 32\n",
    "        pil_image = pil_image.resize((width, height))\n",
    "\n",
    "    if output_type == \"torch\":\n",
    "        perturbed_image = transform(pil_image)\n",
    "    elif output_type == \"numpy\":\n",
    "        perturbed_image = np.array(pil_image)\n",
    "    elif output_type == \"pil\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Unsupported output type provided!\")\n",
    "\n",
    "    return(perturbed_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(box_iou_output):\n",
    "    true_pos = 0\n",
    "    for clean_detection in box_iou_output:\n",
    "        if clean_detection[clean_detection > 0.45].nelement() != 0:\n",
    "            true_pos += 1\n",
    "\n",
    "    return true_pos / box_iou_output.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bbs(image, xyxys):\n",
    "    numpy_clean_img = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    annotator = Annotator(np.ascontiguousarray(numpy_clean_img))\n",
    "    \n",
    "    for xyxy in xyxys:\n",
    "        annotator.box_label(xyxy)\n",
    "    annotator_results = annotator.result()\n",
    "    annotator_results = (annotator_results * 255).astype(np.uint8)\n",
    "    pil_image = Image.fromarray(annotator_results)\n",
    "\n",
    "    return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_bbs(orig_dims, target_dims, orig_xyxy):\n",
    "    # Original image dimensions (640x1200)\n",
    "    original_height, original_width = orig_dims[2:]\n",
    "\n",
    "    # Target image dimensions (640x640)\n",
    "    target_height, target_width = target_dims[2:]\n",
    "\n",
    "    # Calculate scaling factors for x and y dimensions\n",
    "    x_scale = target_width / original_width\n",
    "    y_scale = target_height / original_height\n",
    "\n",
    "    # Scale the coordinates of the bounding boxes\n",
    "    scaled_bounding_boxes = orig_xyxy.clone().detach()  # Create a copy to preserve the original data\n",
    "    scaled_bounding_boxes[:, 0] *= x_scale  # Scale x (left)\n",
    "    scaled_bounding_boxes[:, 1] *= y_scale  # Scale y (top)\n",
    "    scaled_bounding_boxes[:, 2] *= x_scale  # Scale x2 (right)\n",
    "    scaled_bounding_boxes[:, 3] *= y_scale  # Scale y2 (bottom)\n",
    "\n",
    "    return scaled_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(loader = test_loader, Tconf = 0.25, Tiou = 0.45, max_det_pert=300, max_det_clean=300):\n",
    "    all_total_times = []\n",
    "    all_nms_times = []\n",
    "    all_num_bbs_before_nms = []\n",
    "    all_recalls = []\n",
    "\n",
    "    for image_np, _, image_name in tqdm(loader):\n",
    "        perturbed_img = perturb_img(image_name[0],change_aspect_ratio = False).unsqueeze(0) \n",
    "        clean_img = image_np[0].unsqueeze(0)\n",
    "\n",
    "        total_times_single_img = []\n",
    "        nms_times_single_img = []\n",
    "\n",
    "        for i in range(30):\n",
    "            start_time_1 = time.time() * 1000\n",
    "            output_perturbed = model(perturbed_img)[0]\n",
    "            start_time_2 = time.time() * 1000\n",
    "            keep_perturbed = non_max_suppression(output_perturbed, Tconf, Tiou, classes=None,max_det=max_det_pert)\n",
    "            #print(len(keep_perturbed[0]))\n",
    "            end_time = time.time() * 1000 \n",
    "            total_time = end_time - start_time_1\n",
    "            nms_time = end_time - start_time_2\n",
    "            total_times_single_img.append(total_time)\n",
    "            nms_times_single_img.append(nms_time)\n",
    "        \n",
    "        total_time = sum(total_times_single_img) / len(total_times_single_img)\n",
    "        nms_time = sum(nms_times_single_img) / len(nms_times_single_img)\n",
    "\n",
    "        output_clean = model(clean_img)[0]\n",
    "        keep_clean = non_max_suppression(output_clean, Tconf, Tiou, classes=None,max_det=max_det_clean)\n",
    "\n",
    "        num_bbs_before_nms_mask = output_perturbed[..., 4] > Tconf \n",
    "        num_bbs_before_nms = num_bbs_before_nms_mask.sum().item()   #F(C')\n",
    "\n",
    "        clean_xyxy = keep_clean[0][:,0:4]\n",
    "        #print(clean_xyxy)\n",
    "\n",
    "        #with_bbs = add_bbs(clean_img, clean_xyxy)\n",
    "        #display(with_bbs)\n",
    "        perturbed_xyxy = keep_perturbed[0][:,0:4]\n",
    "\n",
    "        # with_bbs = add_bbs(perturbed_img, perturbed_xyxy)\n",
    "        # display(with_bbs)\n",
    "\n",
    "\n",
    "        #scaled_perturbed_xyxy = scale_coords(perturbed_img.shape[2:], perturbed_xyxy, clean_img.shape[2:]).round()\n",
    "        scaled_perturbed_xyxy = scale_bbs(orig_dims=perturbed_img.shape, target_dims=clean_img.shape, orig_xyxy = perturbed_xyxy)\n",
    "\n",
    "        # resized_perturbed_img = F.interpolate(perturbed_img, size=(640,640), mode='bilinear', align_corners=False)\n",
    "        # with_bbs = add_bbs(resized_perturbed_img, scaled_perturbed_xyxy)\n",
    "        # display(with_bbs)\n",
    "\n",
    "        box_iou_output = box_iou(clean_xyxy, scaled_perturbed_xyxy)\n",
    "        \n",
    "        #check if its the case that there aren't any clean detections\n",
    "        #in that case, skip computing the recall\n",
    "        #\"the number of original objects detected in the perturbed image\"\n",
    "        #if there aren't any original objects, no recall can be calculated, thus we skip\n",
    "        if clean_xyxy.nelement() != 0:\n",
    "            #print(box_iou_output)\n",
    "            #break\n",
    "            recall = compute_recall(box_iou_output)\n",
    "            #print(recall)\n",
    "            all_recalls.append(recall)\n",
    "\n",
    "\n",
    "        all_total_times.append(total_time)\n",
    "        all_nms_times.append(nms_time)\n",
    "        all_num_bbs_before_nms.append(num_bbs_before_nms)\n",
    "        \n",
    "    avg_total_time = sum(all_total_times) / len(all_total_times)\n",
    "    avg_nms_time = sum(all_nms_times) / len(all_nms_times)\n",
    "    avg_num_bbs_before_nms = sum(all_num_bbs_before_nms) / len(all_num_bbs_before_nms)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "\n",
    "    return round(avg_total_time, 1), round(avg_nms_time, 1), round(avg_num_bbs_before_nms), round(avg_recall, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_results()\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phantomsponges",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
