{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from local_yolos.yolov5.models.experimental import attempt_load\n",
    "from local_yolos.yolov5.utils.general import non_max_suppression\n",
    "from torchvision import transforms\n",
    "from local_yolos.yolov5.utils.metrics import box_iou\n",
    "from local_yolos.yolov5.utils.plots import Annotator\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "from datasets.augmentations1 import train_transform\n",
    "from datasets.split_data_set_combined import SplitDatasetCombined_BDD\n",
    "from attacks_tools.yolov8_wrapper import CustomPredictor\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script\", flush=True)\n",
    "model_name = 'yolov5'\n",
    "model_size = 's'\n",
    "seed = 42\n",
    "img_size=(640,640)\n",
    "batch_size = 8\n",
    "max_labels_per_img = 65\n",
    "IMG_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val'\n",
    "LAB_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/labels/val'\n",
    "experiment_dir = '/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges/experiments/sample_experiment/'\n",
    "patch_file = experiment_dir + 'final_results/final_patch.png'\n",
    "aspect_ratios = [(1.5, 2, 'width'), (0.7, 0.85, 'height'), (0.6, 0.75, 'both'), (None, None, None)]\n",
    "\n",
    "dir_names = []\n",
    "Path(experiment_dir + '/evaluation').mkdir(parents=True, exist_ok=False)\n",
    "for ar in aspect_ratios:\n",
    "    dir_name = f\"range_scaling=({ar[0]},{ar[1]})_dimension={ar[2]}\"\n",
    "    Path(experiment_dir + '/evaluation/' + dir_name).mkdir(parents=True, exist_ok=False)\n",
    "    dir_names.append(dir_name)\n",
    "\n",
    "Path(experiment_dir + '/evaluation/no_perturbation').mkdir(parents=True, exist_ok=False)\n",
    "dir_names.append('no_perturbation')\n",
    "aspect_ratios.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def set_random_seed(seed_value, use_cuda=True):\n",
    "    np.random.seed(seed_value)  # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu  vars\n",
    "    random.seed(seed_value)  # Python\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # Python hash buildin\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  # needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = SplitDatasetCombined_BDD(\n",
    "            img_dir= IMG_DIR,\n",
    "            lab_dir= LAB_DIR,\n",
    "            max_lab=max_labels_per_img,\n",
    "            img_size=img_size,\n",
    "            transform=train_transform,\n",
    "            collate_fn=collate_fn)\n",
    "\n",
    "train_loader, val_loader, test_loader = split_dataset(val_split=0.1,\n",
    "                                                      shuffle_dataset=True,\n",
    "                                                      random_seed=seed,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      ordered=False,\n",
    "                                                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_yolos/yolov5/weights/yolov5s.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yolov5s summary: 270 layers, 7235389 parameters, 0 gradients, 16.6 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "#model = attempt_load(weights=pt_file).eval()\n",
    "#model = attempt_load_weights('yolov8n.pt', device=torch.device('cuda'), inplace=True, fuse=True).to('cuda')\n",
    "\n",
    "#model = CustomPredictor(overrides = dict(model='yolov8n.pt'))\n",
    "#model = CustomPredictor(weights='local_yolos/yolov5/weights/yolov5s.pt').to('cpu')\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if model_name == 'yolov8':\n",
    "    from local_yolos.yolov8.ultralytics.nn.tasks import attempt_load_weights\n",
    "    from attacks_tools.yolov8_wrapper import CustomDetectionModelWrapper\n",
    "    pt_file = model_name + model_size + '.pt' \n",
    "    model = attempt_load_weights(weights='local_yolos/yolov8/weights/' + pt_file, device=device, inplace=True, fuse=True)\n",
    "    model = CustomDetectionModelWrapper(model)\n",
    "if model_name == 'yolov5':\n",
    "    # taken from https://github.com/ultralytics/yolov5\n",
    "    from local_yolos.yolov5.models.experimental import attempt_load\n",
    "    pt_file = model_name + model_size + '.pt'\n",
    "    model = attempt_load('local_yolos/yolov5/weights/' + pt_file, device).eval()\n",
    "elif model_name == 'yolov4':\n",
    "    # taken from https://github.com/WongKinYiu/PyTorch_YOLOv4\n",
    "    from local_yolos.yolov4.models.models import Darknet, load_darknet_weights\n",
    "    model = Darknet('local_yolos/yolov4/cfg/yolov4.cfg', img_size=640).to(device).eval()\n",
    "    load_darknet_weights(model, 'local_yolos/yolov4/weights/yolov4.weights')\n",
    "elif model_name == 'yolov3':\n",
    "    # taken from https://github.com/ultralytics/yolov3\n",
    "    from local_yolos.yolov3 import hubconf\n",
    "    model = hubconf.yolov3(pretrained=True, autoshape=False, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_img(img_name, aspect_ratio = None, output_type = \"torch\"):\n",
    "    image_path = IMG_DIR + \"/\" + img_name\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((640, 640))\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    clean_img = transform(image)\n",
    "\n",
    "    # Load the image using PIL\n",
    "    #patch_path =  \"/home/cschoof/experiments/yolov_5_epsilon=30_lambda1=0.6_lambda2=10/final_results/final_patch.png\"\n",
    "    patch_path = patch_file\n",
    "    patch = Image.open(patch_path)\n",
    "    patch = transform(patch)\n",
    "\n",
    "    perturbed_img = torch.clamp(clean_img + patch, 0, 1)\n",
    "\n",
    "    # Assuming you have a torch tensor named torch_image\n",
    "    # You can resize the tensor to (H, W, C) shape using transpose and multiply by 255 (if it's not already in the correct range)\n",
    "    #pil_image = transforms.ToPILImage()(result.squeeze().cpu() * 255)\n",
    "    tensor_min = torch.min(perturbed_img)\n",
    "    tensor_max = torch.max(perturbed_img)\n",
    "    normalized_tensor = (perturbed_img - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Convert the normalized tensor to a PIL image\n",
    "    pil_image = transforms.ToPILImage()(normalized_tensor.squeeze().cpu())\n",
    "    random_scale_w = 1\n",
    "    random_scale_h = 1\n",
    "\n",
    "    if aspect_ratio[2] == 'width':\n",
    "        random_scale_w = random.uniform(aspect_ratio[0], aspect_ratio[1])\n",
    "    elif aspect_ratio[2] == 'height':\n",
    "        random_scale_h = random.uniform(aspect_ratio[0], aspect_ratio[1])\n",
    "    elif aspect_ratio[2] == 'both':\n",
    "        random_scale_w = random.uniform(aspect_ratio[0], aspect_ratio[1])\n",
    "        random_scale_h = random.uniform(aspect_ratio[0], aspect_ratio[1])\n",
    "        \n",
    "\n",
    "    width = int(640 * random_scale_w)\n",
    "    width = (width // 32) * 32 #ensures that the new width is a multiple of 32, otherwise the model does not work\n",
    "    height = int(640 * random_scale_h)\n",
    "    height = (height // 32) * 32\n",
    "    pil_image = pil_image.resize((width, height))\n",
    "\n",
    "    if output_type == \"torch\":\n",
    "        perturbed_image = transform(pil_image)\n",
    "    elif output_type == \"numpy\":\n",
    "        perturbed_image = np.array(pil_image)\n",
    "    elif output_type == \"pil\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Unsupported output type provided!\")\n",
    "\n",
    "    return(perturbed_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(box_iou_output):\n",
    "    true_pos = 0\n",
    "    for clean_detection in box_iou_output:\n",
    "        if clean_detection[clean_detection > 0.45].nelement() != 0:\n",
    "            true_pos += 1\n",
    "\n",
    "    return true_pos / box_iou_output.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bbs(image, xyxys):\n",
    "    numpy_clean_img = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    annotator = Annotator(np.ascontiguousarray(numpy_clean_img))\n",
    "    \n",
    "    for xyxy in xyxys:\n",
    "        annotator.box_label(xyxy)\n",
    "    annotator_results = annotator.result()\n",
    "    annotator_results = (annotator_results * 255).astype(np.uint8)\n",
    "    pil_image = Image.fromarray(annotator_results)\n",
    "\n",
    "    return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_bbs(orig_dims, target_dims, orig_xyxy):\n",
    "    # Original image dimensions (640x1200)\n",
    "    original_height, original_width = orig_dims[2:]\n",
    "\n",
    "    # Target image dimensions (640x640)\n",
    "    target_height, target_width = target_dims[2:]\n",
    "\n",
    "    # Calculate scaling factors for x and y dimensions\n",
    "    x_scale = target_width / original_width\n",
    "    y_scale = target_height / original_height\n",
    "\n",
    "    # Scale the coordinates of the bounding boxes\n",
    "    scaled_bounding_boxes = orig_xyxy.clone().detach()  # Create a copy to preserve the original data\n",
    "    scaled_bounding_boxes[:, 0] *= x_scale  # Scale x (left)\n",
    "    scaled_bounding_boxes[:, 1] *= y_scale  # Scale y (top)\n",
    "    scaled_bounding_boxes[:, 2] *= x_scale  # Scale x2 (right)\n",
    "    scaled_bounding_boxes[:, 3] *= y_scale  # Scale y2 (bottom)\n",
    "\n",
    "    return scaled_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(loader = test_loader, Tconf = 0.25, Tiou = 0.45, max_det_pert=300, max_det_clean=300, save_png_interval = 25):\n",
    "    results_for_all_aspect_ratios = []\n",
    "    for aspect_ratio, dir_name in zip(aspect_ratios, dir_names):\n",
    "        print(dir_name)\n",
    "\n",
    "        all_total_times = []\n",
    "        all_nms_times = []\n",
    "        all_num_bbs_before_nms = []\n",
    "        all_recalls = []\n",
    "        results_for_single_aspect_ratio_dir = dir_name\n",
    "\n",
    "        for idx, (image_np, _, image_name) in tqdm(enumerate(loader)):\n",
    "            if idx == 10:\n",
    "                break\n",
    "            if dir_name != 'no_perturbation':\n",
    "                perturbed_img = perturb_img(image_name[0], aspect_ratio=aspect_ratio).unsqueeze(0) \n",
    "            else:\n",
    "                perturbed_img = image_np[0].unsqueeze(0)\n",
    "            clean_img = image_np[0].unsqueeze(0)\n",
    "\n",
    "            total_times_single_img = []\n",
    "            nms_times_single_img = []\n",
    "\n",
    "            for i in range(1):\n",
    "                start_time_1 = time.time() * 1000\n",
    "                output_perturbed = model(perturbed_img)[0]\n",
    "                start_time_2 = time.time() * 1000\n",
    "                keep_perturbed = non_max_suppression(output_perturbed, Tconf, Tiou, classes=None,max_det=max_det_pert)\n",
    "                #print(len(keep_perturbed[0]))\n",
    "                end_time = time.time() * 1000 \n",
    "                total_time = end_time - start_time_1\n",
    "                nms_time = end_time - start_time_2\n",
    "                total_times_single_img.append(total_time)\n",
    "                nms_times_single_img.append(nms_time)\n",
    "            \n",
    "            total_time = sum(total_times_single_img) / len(total_times_single_img)\n",
    "            nms_time = sum(nms_times_single_img) / len(nms_times_single_img)\n",
    "\n",
    "            output_clean = model(clean_img)[0]\n",
    "            keep_clean = non_max_suppression(output_clean, Tconf, Tiou, classes=None,max_det=max_det_clean)\n",
    "\n",
    "            num_bbs_before_nms_mask = output_perturbed[..., 4] > Tconf \n",
    "            num_bbs_before_nms = num_bbs_before_nms_mask.sum().item()   #F(C')\n",
    "\n",
    "            clean_xyxy = keep_clean[0][:,0:4]\n",
    "            #print(clean_xyxy)\n",
    "\n",
    "            if idx % save_png_interval == 0:\n",
    "                with_bbs = add_bbs(clean_img, clean_xyxy)\n",
    "                #display(with_bbs)\n",
    "                image_name = image_name[0].split('.')[0]\n",
    "                output_path =  experiment_dir + '/evaluation/' + results_for_single_aspect_ratio_dir + '/' + f\"{image_name}_cleanImg\"\n",
    "                with_bbs.save(output_path+'.png')\n",
    "\n",
    "\n",
    "            perturbed_xyxy = keep_perturbed[0][:,0:4]\n",
    "            \n",
    "            if idx % save_png_interval == 0:\n",
    "                with_bbs = add_bbs(perturbed_img, perturbed_xyxy)\n",
    "                #display(with_bbs)\n",
    "                output_path = experiment_dir + '/evaluation/' + results_for_single_aspect_ratio_dir + '/' + f\"{image_name}_pertImg\"\n",
    "                with_bbs.save(output_path+'.png')\n",
    "\n",
    "            scaled_perturbed_xyxy = scale_bbs(orig_dims=perturbed_img.shape, target_dims=clean_img.shape, orig_xyxy = perturbed_xyxy)\n",
    "\n",
    "            # resized_perturbed_img = F.interpolate(perturbed_img, size=(640,640), mode='bilinear', align_corners=False)\n",
    "            # with_bbs = add_bbs(resized_perturbed_img, scaled_perturbed_xyxy)\n",
    "            # display(with_bbs)\n",
    "\n",
    "            box_iou_output = box_iou(clean_xyxy, scaled_perturbed_xyxy)\n",
    "            \n",
    "            #check if its the case that there aren't any clean detections\n",
    "            #in that case, skip computing the recall\n",
    "            #\"the number of original objects detected in the perturbed image\"\n",
    "            #if there aren't any original objects, no recall can be calculated, thus we skip\n",
    "            if clean_xyxy.nelement() != 0:\n",
    "                #print(box_iou_output)\n",
    "                #break\n",
    "                recall = compute_recall(box_iou_output)\n",
    "                #print(recall)\n",
    "                all_recalls.append(recall)\n",
    "\n",
    "            all_total_times.append(total_time)\n",
    "            all_nms_times.append(nms_time)\n",
    "            all_num_bbs_before_nms.append(num_bbs_before_nms)\n",
    "            \n",
    "        avg_total_time = sum(all_total_times) / len(all_total_times)\n",
    "        avg_nms_time = sum(all_nms_times) / len(all_nms_times)\n",
    "        avg_num_bbs_before_nms = sum(all_num_bbs_before_nms) / len(all_num_bbs_before_nms)\n",
    "        avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "\n",
    "        results_for_all_aspect_ratios.append((round(avg_total_time, 1), round(avg_nms_time, 1), round(avg_num_bbs_before_nms), round(avg_recall, 3))) \n",
    "    return results_for_all_aspect_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range_scaling=(1.5,2)_dimension=width\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:15,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range_scaling=(0.7,0.85)_dimension=height\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:10,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range_scaling=(0.6,0.75)_dimension=both\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:09,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range_scaling=(None,None)_dimension=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:10,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:10,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(aspect_ratios, results, csv_file_path):\n",
    "    # Create a list of dictionaries for CSV data\n",
    "    csv_data = [{\"aspect_ratios\": pair_2 if type(pair_2) == tuple else 'no_pert', \n",
    "                \"inference_time\": pair_4[0], \n",
    "                \"nms_time\": pair_4[1], \n",
    "                \"num_bbs_before_nms\": pair_4[2], \n",
    "                \"recall\": pair_4[3]} for pair_2, pair_4 in zip(aspect_ratios, results)]\n",
    "\n",
    "    # Save the data as CSV\n",
    "    with open(csv_file_path, \"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"aspect_ratios\", \"inference_time\", \"nms_time\", \"num_bbs_before_nms\", \"recall\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_data)\n",
    "\n",
    "    print(f\"Data has been saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to /Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges/experiments/sample_experiment/evaluation/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "save_csv(aspect_ratios, results, experiment_dir + 'evaluation' + '/' + 'evaluation_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phantomsponges",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
