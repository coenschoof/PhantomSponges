{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "configuration:\n",
    "models_ver - insert YOLO version's numbers that the UAP will be trained on.\n",
    "\n",
    "epsilon, lambda_1, lambda_2 - attack's parameters. more information can be found in the [paper](https://arxiv.org/abs/2205.13618)\n",
    "\n",
    "BDD_IMG_DIR - a path to the BDD validation set images (or any other wanted dataset)\n",
    "\n",
    "BDD_LAB_DIR - a path to the BDD validation set labels (or any other wanted dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models_vers = [8] # for example: models_vers = [5] or models_vers = [3, 4, 5]\n",
    "epsilon = 70\n",
    "lambda_1 = 1\n",
    "lambda_2 = 10\n",
    "seed = 42\n",
    "patch_size=(640,640)\n",
    "img_size=(640,640)\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "max_labels_per_img = 65\n",
    "BDD_IMG_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val'\n",
    "BDD_LAB_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/labels/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load BDD dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coenschoof/miniconda/envs/phantomsponges/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "from datasets.augmentations1 import train_transform\n",
    "from datasets.split_data_set_combined import SplitDatasetCombined_BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# batch = [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "\n",
    "# unzipped = zip(*batch)\n",
    "\n",
    "# numbers, letters = unzipped\n",
    "\n",
    "# print(numbers)  # Output: (1, 2, 3)\n",
    "# print(letters)  # Output: ('a', 'b', 'c')\n",
    "\n",
    "#dus ipv [(tensor, array, string), (tensor, array, string), (tensor, array, string)] hebben we\n",
    "# train_loader[0] bevat 8 images\n",
    "# train_loader[1] bevat alle labels per voor 8 images\n",
    "# train_loader[2] bevat paths naar 8 images\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def set_random_seed(seed_value, use_cuda=True):\n",
    "    numpy.random.seed(seed_value)  # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu  vars\n",
    "    random.seed(seed_value)  # Python\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # Python hash buildin\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  # needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = SplitDatasetCombined_BDD(\n",
    "            img_dir= BDD_IMG_DIR,\n",
    "            lab_dir= BDD_LAB_DIR,\n",
    "            max_lab=max_labels_per_img,\n",
    "            img_size=img_size,\n",
    "            transform=train_transform,\n",
    "            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader en val_loader \n",
    "\n",
    "#train loader bevat een tuple van 3, elk element van de tuple bevat 8 samples (8 samples maakt 1 batch)\n",
    "#De tuple bestaat uit het volgende:\n",
    "    #[0] = de image met size (3,640,640)\n",
    "    #[1] = de bounding boxes met size (x, 5) (5 bestaat uit class, x, y, x, y)\n",
    "        #lijkt nergens gebruikt te worden\n",
    "    #[2] = de naam van de image (e.g. 'c98258a4-54a47ff2.jpg')  \n",
    "\n",
    "    #HEB IK AL DEZE INFORMATIE OOK NODIG WANNEER IK WIL TESTEN OP PASCAL/MTSD? converten is een pain\n",
    "        #WORDT [2] ERGENS GEBRUIKT? IK WEET DAT [1] NERGENS GEBRUIKT WORDT\n",
    "train_loader, val_loader, test_loader = split_dataset(val_split=0.1,\n",
    "                                                      shuffle_dataset=True,\n",
    "                                                      random_seed=seed,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      ordered=False,\n",
    "                                                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from attack.uap_phantom_spong_for_yolov8 import UAPPhantomSponge\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "patch_name = r\"yolov\"\n",
    "for ver in models_vers:\n",
    "  patch_name += f\"_{ver}\"\n",
    "patch_name += f\"_epsilon={epsilon}_lambda1={lambda_1}_lambda2={lambda_2}\"\n",
    "\n",
    "uap_phantom_sponge_attack = UAPPhantomSponge(patch_folder=patch_name, \n",
    "                                             train_loader=train_loader, \n",
    "                                             val_loader=val_loader, \n",
    "                                             epsilon = epsilon, \n",
    "                                             lambda_1=lambda_1, \n",
    "                                             lambda_2=lambda_2, \n",
    "                                             patch_size=patch_size, \n",
    "                                             models_vers=models_vers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "saving png of current patch at epoch: 0, batch: 0/169...\n",
      "66965\n",
      "TEST\n",
      "torch.Size([8, 8400])\n",
      "batch average number (/8) of bbs that will be passed to the NMS stage: 29.375\n",
      "tensor([0.2500, 0.2500, 0.2500,  ..., 0.2498, 0.2499, 0.2500], grad_fn=<MaximumBackward>)\n",
      "combined_loss, max_obj, bb_area, iou_loss for this batch =               0.438|0.248|0.019|0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m adv_img \u001b[39m=\u001b[39m uap_phantom_sponge_attack\u001b[39m.\u001b[39;49mrun_attack()\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/PhantomSponges/attack/uap_phantom_spong_for_yolov8.py:711\u001b[0m, in \u001b[0;36mUAPPhantomSponge.run_attack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_attack\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 711\u001b[0m     tensor_adv_patch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpgd_L2(epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon, iter_eps\u001b[39m=\u001b[39;49m\u001b[39m0.0005\u001b[39;49m) \n\u001b[1;32m    713\u001b[0m     patch \u001b[39m=\u001b[39m tensor_adv_patch\n\u001b[1;32m    715\u001b[0m     \u001b[39m#schiet een plaatje van de final adv patch\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m#slaat ook de train/val losses etc per epoch op en stopt ieder in een lijst\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/PhantomSponges/attack/uap_phantom_spong_for_yolov8.py:659\u001b[0m, in \u001b[0;36mUAPPhantomSponge.pgd_L2\u001b[0;34m(self, epsilon, iter_eps, min_x, max_x)\u001b[0m\n\u001b[1;32m    652\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(imgs)\n\u001b[1;32m    654\u001b[0m \u001b[39m#returnt middels FGSM een perturbed plaatje van 3x640x640\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[39m#adv_patch = bij epoch 0, batch 0 is dit een zwart plaatje (tensor met 0's)\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[39m#x = één tensor batch van 8 plaatjes\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[39m#label = een tuple van 8, elke entry bevat een np.array met elke row een bb, en columns [class, x-center, y-center, w, h]\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[39m#iter_eps = 0.0005, bepaalt de sterkte van de perturbation\u001b[39;00m\n\u001b[0;32m--> 659\u001b[0m adv_patch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfastGradientSignMethod(adv_patch, x, label, epsilon\u001b[39m=\u001b[39;49miter_eps)\n\u001b[1;32m    660\u001b[0m \u001b[39m#torch.save(adv_patch, f'self.full_patch_folder + tensor_{i}.pt')\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m#print(f\"adv_patch: {adv_patch}\")s\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \n\u001b[1;32m    663\u001b[0m \u001b[39m# Project the perturbation to the epsilon ball (L2 projection)\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[39m# Neem het verschil tussen het zwarte plaatje en het perturbed zwarte plaatje\u001b[39;00m\n\u001b[1;32m    665\u001b[0m perturbation \u001b[39m=\u001b[39m adv_patch \u001b[39m-\u001b[39m patch\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/PhantomSponges/attack/uap_phantom_spong_for_yolov8.py:586\u001b[0m, in \u001b[0;36mUAPPhantomSponge.fastGradientSignMethod\u001b[0;34m(self, adv_patch, images, labels, epsilon)\u001b[0m\n\u001b[1;32m    580\u001b[0m penalty_term \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_penalty_term(images, images)\n\u001b[1;32m    581\u001b[0m \u001b[39m#applied_patch = batch + perturbation\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[39m#images = images van de batch\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[39m#labels = labels van de batch\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[39m#penatly_term = 0\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[39m#adv_patch = zeroes van 3x640x640\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m data_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_function_gradient(applied_patch, images, labels, penalty_term,\n\u001b[1;32m    587\u001b[0m                                         adv_patch) \n\u001b[1;32m    589\u001b[0m \u001b[39m# Collect the element-wise sign of the data gradient\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[39m#returnt -1, 0 of 1\u001b[39;00m\n\u001b[1;32m    591\u001b[0m sign_data_grad \u001b[39m=\u001b[39m data_grad\u001b[39m.\u001b[39msign()\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/PhantomSponges/attack/uap_phantom_spong_for_yolov8.py:535\u001b[0m, in \u001b[0;36mUAPPhantomSponge.loss_function_gradient\u001b[0;34m(self, applied_patch, init_images, batch_label, penalty_term, adv_patch)\u001b[0m\n\u001b[1;32m    531\u001b[0m r \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# choose a random model\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    534\u001b[0m     \u001b[39m#WAT OUTPUT HET MODEL PRECIES?\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     output_clean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodels[r](init_images)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m    536\u001b[0m output_patch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels[r](applied_patch)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m max_objects_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_objects(output_patch)\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/PhantomSponges/attacks_tools/attack_utils_2.py:11\u001b[0m, in \u001b[0;36mCustomPredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel:\n\u001b[1;32m      8\u001b[0m     \u001b[39m#print(model)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup_model(model, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(source))\n\u001b[1;32m     12\u001b[0m \u001b[39m#print(self.model.model(source))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m preds[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m preds[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m) \u001b[39m#make sure its similar to yolov5's output (torch.Size([8, 25200, 85]))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/ultralytics/nn/tasks.py:45\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/ultralytics/nn/tasks.py:62\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/ultralytics/nn/tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 82\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m     83\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/ultralytics/nn/modules/head.py:47\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m shape \u001b[39m=\u001b[39m x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape  \u001b[39m# BCHW\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnl):\n\u001b[0;32m---> 47\u001b[0m     x[i] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2[i](x[i]), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv3[i](x[i])), \u001b[39m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/ultralytics/nn/modules/conv.py:42\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     41\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 443\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda/envs/phantomsponges/lib/python3.9/site-packages/torch/nn/modules/conv.py:439\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    437\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    438\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 439\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    440\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adv_img = uap_phantom_sponge_attack.run_attack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE FOR EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from local_yolos.yolov5.models.experimental import attempt_load\n",
    "from local_yolos.yolov5.utils.general import non_max_suppression\n",
    "from torchvision import transforms\n",
    "from local_yolos.yolov5.utils.metrics import box_iou\n",
    "from local_yolos.yolov5.utils.plots import Annotator\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_img(img_name, change_aspect_ratio = True, output_type = \"torch\", patch_name = 'final_patch.png'):\n",
    "    image_path = \"/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val/\" + img_name\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((640, 640))\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    clean_img = transform(image)\n",
    "\n",
    "    # Load the image using PIL\n",
    "    patch_path =  \"/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges/final_patch.png\"\n",
    "    patch = Image.open(patch_path)\n",
    "    patch = transform(patch)\n",
    "\n",
    "    perturbed_img = torch.clamp(clean_img + patch, 0, 1)\n",
    "\n",
    "    # Assuming you have a torch tensor named torch_image\n",
    "    # You can resize the tensor to (H, W, C) shape using transpose and multiply by 255 (if it's not already in the correct range)\n",
    "    #pil_image = transforms.ToPILImage()(result.squeeze().cpu() * 255)\n",
    "    tensor_min = torch.min(perturbed_img)\n",
    "    tensor_max = torch.max(perturbed_img)\n",
    "    normalized_tensor = (perturbed_img - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Convert the normalized tensor to a PIL image\n",
    "    pil_image = transforms.ToPILImage()(normalized_tensor.squeeze().cpu())\n",
    "    if change_aspect_ratio:\n",
    "        random_scale_w = random.uniform(0.7, 0.9) #1.5, 3\n",
    "        #random_scale_h = random.uniform(1, 2.5)\n",
    "        width = int(640 * random_scale_w)\n",
    "        width = (width // 32) * 32 #ensures that the new width is a multiple of 32, otherwise the model does not work\n",
    "        height = 640 #height = int(640 * random_scale_h)\n",
    "        height = (height // 32) * 32\n",
    "        pil_image = pil_image.resize((width, height))\n",
    "\n",
    "    if output_type == \"torch\":\n",
    "        perturbed_image = transform(pil_image)\n",
    "    elif output_type == \"numpy\":\n",
    "        perturbed_image = np.array(pil_image)\n",
    "    elif output == \"pil\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Unsupported output type provided!\")\n",
    "\n",
    "    return(perturbed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:08<00:00, 60.14it/s]\n"
     ]
    }
   ],
   "source": [
    "image_names = []\n",
    "for image_np, _, image_name in tqdm(test_loader):\n",
    "    image_names.append(image_name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New loss term experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tiny batch containing 3 images; 2 random images from the test set, and a fully white image\n",
    "\n",
    "zeros_tensor = torch.zeros(1, 3, 640, 640)\n",
    "#zero_output = model(zeros_tensor)[0]\n",
    "\n",
    "random_index = random.randrange(len(image_names))\n",
    "other_image = perturb_img(image_names[random_index], change_aspect_ratio = False, output_type = \"torch\").unsqueeze(0)\n",
    "random_index = random.randrange(len(image_names))\n",
    "other_image_2 = perturb_img(image_names[random_index], change_aspect_ratio = False, output_type = \"torch\").unsqueeze(0)\n",
    "pil_image = transforms.ToPILImage()(other_image.squeeze())\n",
    "#display(pil_image)\n",
    "\n",
    "tiny_batch = torch.cat((other_image, other_image_2, zeros_tensor))\n",
    "#tiny_batch_output = model(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_yolos.yolov8.ultralytics.models.yolo.model import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from attacks_tools.attack_utils import CustomPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runs Non-Maximum Suppression (NMS) on inference results\\n\\nReturns:\\n        list of detections, on (n,6) tensor per image [xyxy, conf, cls]\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ONDERSTAANDE OUTPUT MOET NAAR DEZE SHAPE WORDEN OMGEZET\n",
    "\n",
    "\"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n",
    "\n",
    "Returns:\n",
    "        list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "\"\"\"\n",
    "\n",
    "#output_patch.size() = torch.Size([8, 25200, 85])\n",
    "    #8 plaatjes per batch\n",
    "    #25200 bounding boxes per plaatje\n",
    "    #85 = (x, y, x, y), object confidence score, and class probabilities. (dus 80 class probs)\n",
    "#print(output_patch.size())\n",
    "\n",
    "# predictor = CustomPredictor(overrides = dict(model='yolov8n.pt') )\n",
    "# results = predictor(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(predictor.model))\n",
    "# print(type(predictor))\n",
    "# print(type(predictor.model.zero_grad()))\n",
    "#print(type(predictor.zero_grad()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor(tiny_batch)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.model(tiny_batch)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have the original tensor 'results[0]'\n",
    "# original_tensor = results[0]\n",
    "\n",
    "# # Create a new tensor with the desired shape 'torch.Size([3, 8400, 85])'\n",
    "# new_shape = list(original_tensor.shape)\n",
    "# new_shape[-1] += 1\n",
    "# new_tensor = torch.zeros(new_shape)\n",
    "\n",
    "# # Copy the data from the original tensor to the new tensor, shifting values to the right\n",
    "# new_tensor[:, :, 4] = 1 \n",
    "# new_tensor[:, :, 0:4], new_tensor[:, :, 5:] = original_tensor[:, :, 0:4], original_tensor[:, :, 4:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_tensor[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tensor[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2 = new_tensor[:, :, 5:] * new_tensor[:, :, 4:5]\n",
    "# conf, _ = x2.max(2, keepdim=False) \n",
    "# print(conf.shape)\n",
    "# target_class = 2\n",
    "# all_target_conf = x2[:, :, target_class]\n",
    "# print(all_target_conf.shape)\n",
    "# under_thr_target_conf = all_target_conf[conf < 0.25]\n",
    "# print(under_thr_target_conf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tensor[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_yolos.yolov8.ultralytics.utils.ops import non_max_suppression\n",
    "# non_max_suppression(results[0])[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "from attacks_tools.attack_utils_2 import CustomPredictor\n",
    "predictor = CustomPredictor(weights='yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_patch = predictor(tiny_batch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch average number (/8) of bbs that will be passed to the NMS stage: 27.333333333333332\n",
      "tensor([0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500], grad_fn=<MaximumBackward>)\n"
     ]
    }
   ],
   "source": [
    "x2 = output_patch[:, :, 5:] * output_patch[:, :, 4:5] #torch.Size([8, 25200, 80])     #eq. 2 in paper\n",
    "#neem per plaatje, per bb, de max confidence.\n",
    "#We houden dus van 80 bbs 1 class over die de hoogste conf heeft, we slaan de conf scores hier op, niet de bbs of de class\n",
    "conf, _ = x2.max(2, keepdim=False) #torch.Size([8, 25200])\n",
    "#neem van alle plaatjes in de batch (8), van alle bounding boxes per plaatje (25200)..\n",
    "#alleen de bounding box met class \"car\" (dat is index 2 van de 80)\n",
    "all_target_conf = x2[:, :, 2] #torch.Size([8, 25200])\n",
    "under_thr_target_conf = all_target_conf[conf < 0.25]\n",
    "conf_avg = len(conf.view(-1)[conf.view(-1) > 0.25]) / len(output_patch)\n",
    "print(f\"batch average number (/8) of bbs that will be passed to the NMS stage: {conf_avg}\")\n",
    "zeros = torch.zeros(under_thr_target_conf.size()).to(output_patch.device) #size is rondom torch.Size([200000])\n",
    "zeros.requires_grad = True\n",
    "\n",
    "x3 = torch.maximum(0.25 - under_thr_target_conf, zeros) #eq. 3 in paper #size is rondom torch.Size([200000])\n",
    "#±200000 gedeeld door 201600\n",
    "print(x3) #BIJNA ALLEMAAL 0.25!!!!!!!\n",
    "mean_conf = torch.sum(x3, dim=0) / (output_patch.size()[0] * output_patch.size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_yolos.yolov8.ultralytics.nn.tasks import attempt_load_weights\n",
    "model = attempt_load_weights('yolov8n.pt',\n",
    "                                #device=device,\n",
    "                                inplace=True,\n",
    "                                #fuse=fuse\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0816e+01, 9.5026e+00, 2.2359e+01,  ..., 3.8697e-07, 2.0936e-07, 2.0073e-07],\n",
       "        [1.6403e+01, 7.1494e+00, 1.3847e+01,  ..., 3.0834e-07, 2.0266e-07, 2.0283e-07],\n",
       "        [1.7717e+01, 9.6397e+00, 1.6733e+01,  ..., 2.6557e-07, 1.3718e-07, 1.6254e-07],\n",
       "        ...,\n",
       "        [4.6013e+02, 6.2085e+02, 3.5701e+02,  ..., 1.2191e-05, 4.3941e-06, 7.7318e-06],\n",
       "        [5.9507e+02, 6.1933e+02, 8.8169e+01,  ..., 2.8870e-05, 7.4756e-06, 9.2373e-06],\n",
       "        [6.0427e+02, 6.1762e+02, 7.1005e+01,  ..., 6.4021e-05, 1.1112e-05, 1.3656e-05]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor(tiny_batch)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0816e+01, 1.6403e+01, 1.7717e+01,  ..., 4.6013e+02, 5.9507e+02, 6.0427e+02],\n",
       "        [9.5026e+00, 7.1494e+00, 9.6397e+00,  ..., 6.2085e+02, 6.1933e+02, 6.1762e+02],\n",
       "        [2.2359e+01, 1.3847e+01, 1.6733e+01,  ..., 3.5701e+02, 8.8169e+01, 7.1005e+01],\n",
       "        ...,\n",
       "        [3.8696e-07, 3.0834e-07, 2.6557e-07,  ..., 1.2191e-05, 2.8870e-05, 6.4022e-05],\n",
       "        [2.0936e-07, 2.0266e-07, 1.3718e-07,  ..., 4.3940e-06, 7.4757e-06, 1.1112e-05],\n",
       "        [2.0073e-07, 2.0283e-07, 1.6254e-07,  ..., 7.7317e-06, 9.2372e-06, 1.3656e-05]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tiny_batch)[0][0] #DIMENSIES ZIJN GEFLIPT? LIJKT ME NIET DE BEDOELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "<class 'attacks_tools.attack_utils_2.CustomPredictor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(predictor.zero_grad()))\n",
    "print(type(predictor.model))\n",
    "print(type(predictor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
