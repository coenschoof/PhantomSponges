{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "configuration:\n",
    "models_ver - insert YOLO version's numbers that the UAP will be trained on.\n",
    "\n",
    "epsilon, lambda_1, lambda_2 - attack's parameters. more information can be found in the [paper](https://arxiv.org/abs/2205.13618)\n",
    "\n",
    "BDD_IMG_DIR - a path to the BDD validation set images (or any other wanted dataset)\n",
    "\n",
    "BDD_LAB_DIR - a path to the BDD validation set labels (or any other wanted dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models_vers = [8] # for example: models_vers = [5] or models_vers = [3, 4, 5]\n",
    "epsilon = 70\n",
    "lambda_1 = 1\n",
    "lambda_2 = 10\n",
    "seed = 42\n",
    "patch_size=(640,640)\n",
    "img_size=(640,640)\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "max_labels_per_img = 65\n",
    "BDD_IMG_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val'\n",
    "BDD_LAB_DIR = '/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/labels/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load BDD dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coenschoof/miniconda/envs/phantomsponges/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "from datasets.augmentations1 import train_transform\n",
    "from datasets.split_data_set_combined import SplitDatasetCombined_BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# batch = [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "\n",
    "# unzipped = zip(*batch)\n",
    "\n",
    "# numbers, letters = unzipped\n",
    "\n",
    "# print(numbers)  # Output: (1, 2, 3)\n",
    "# print(letters)  # Output: ('a', 'b', 'c')\n",
    "\n",
    "#dus ipv [(tensor, array, string), (tensor, array, string), (tensor, array, string)] hebben we\n",
    "# train_loader[0] bevat 8 images\n",
    "# train_loader[1] bevat alle labels per voor 8 images\n",
    "# train_loader[2] bevat paths naar 8 images\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def set_random_seed(seed_value, use_cuda=True):\n",
    "    numpy.random.seed(seed_value)  # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu  vars\n",
    "    random.seed(seed_value)  # Python\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # Python hash buildin\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  # needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = SplitDatasetCombined_BDD(\n",
    "            img_dir= BDD_IMG_DIR,\n",
    "            lab_dir= BDD_LAB_DIR,\n",
    "            max_lab=max_labels_per_img,\n",
    "            img_size=img_size,\n",
    "            transform=train_transform,\n",
    "            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader en val_loader \n",
    "\n",
    "#train loader bevat een tuple van 3, elk element van de tuple bevat 8 samples (8 samples maakt 1 batch)\n",
    "#De tuple bestaat uit het volgende:\n",
    "    #[0] = de image met size (3,640,640)\n",
    "    #[1] = de bounding boxes met size (x, 5) (5 bestaat uit class, x, y, x, y)\n",
    "        #lijkt nergens gebruikt te worden\n",
    "    #[2] = de naam van de image (e.g. 'c98258a4-54a47ff2.jpg')  \n",
    "\n",
    "    #HEB IK AL DEZE INFORMATIE OOK NODIG WANNEER IK WIL TESTEN OP PASCAL/MTSD? converten is een pain\n",
    "        #WORDT [2] ERGENS GEBRUIKT? IK WEET DAT [1] NERGENS GEBRUIKT WORDT\n",
    "train_loader, val_loader, test_loader = split_dataset(val_split=0.1,\n",
    "                                                      shuffle_dataset=True,\n",
    "                                                      random_seed=seed,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      ordered=False,\n",
    "                                                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# from attack.uap_phantom_sponge_yolov8 import UAPPhantomSponge\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# patch_name = r\"yolov\"\n",
    "# for ver in models_vers:\n",
    "#   patch_name += f\"_{ver}\"\n",
    "# patch_name += f\"_epsilon={epsilon}_lambda1={lambda_1}_lambda2={lambda_2}\"\n",
    "\n",
    "# uap_phantom_sponge_attack = UAPPhantomSponge(patch_folder=patch_name, \n",
    "#                                              train_loader=train_loader, \n",
    "#                                              val_loader=val_loader, \n",
    "#                                              epsilon = epsilon, \n",
    "#                                              lambda_1=lambda_1, \n",
    "#                                              lambda_2=lambda_2, \n",
    "#                                              patch_size=patch_size, \n",
    "#                                              models_vers=models_vers)\n",
    "# #adv_img = uap_phantom_sponge_attack.run_attack()sdfsfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE FOR EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from local_yolos.yolov5.models.experimental import attempt_load\n",
    "from local_yolos.yolov5.utils.general import non_max_suppression\n",
    "from torchvision import transforms\n",
    "from local_yolos.yolov5.utils.metrics import box_iou\n",
    "from local_yolos.yolov5.utils.plots import Annotator\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_img(img_name, change_aspect_ratio = True, output_type = \"torch\", patch_name = 'final_patch.png'):\n",
    "    image_path = \"/Users/coenschoof/miniconda/envs/phantomsponges/BDD100K-to-YOLOV5/bdd_in_YOLOV5_train_newLabels/images/val/\" + img_name\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((640, 640))\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    clean_img = transform(image)\n",
    "\n",
    "    # Load the image using PIL\n",
    "    patch_path =  \"/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges/final_patch.png\"\n",
    "    patch = Image.open(patch_path)\n",
    "    patch = transform(patch)\n",
    "\n",
    "    perturbed_img = torch.clamp(clean_img + patch, 0, 1)\n",
    "\n",
    "    # Assuming you have a torch tensor named torch_image\n",
    "    # You can resize the tensor to (H, W, C) shape using transpose and multiply by 255 (if it's not already in the correct range)\n",
    "    #pil_image = transforms.ToPILImage()(result.squeeze().cpu() * 255)\n",
    "    tensor_min = torch.min(perturbed_img)\n",
    "    tensor_max = torch.max(perturbed_img)\n",
    "    normalized_tensor = (perturbed_img - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Convert the normalized tensor to a PIL image\n",
    "    pil_image = transforms.ToPILImage()(normalized_tensor.squeeze().cpu())\n",
    "    if change_aspect_ratio:\n",
    "        random_scale_w = random.uniform(0.7, 0.9) #1.5, 3\n",
    "        #random_scale_h = random.uniform(1, 2.5)\n",
    "        width = int(640 * random_scale_w)\n",
    "        width = (width // 32) * 32 #ensures that the new width is a multiple of 32, otherwise the model does not work\n",
    "        height = 640 #height = int(640 * random_scale_h)\n",
    "        height = (height // 32) * 32\n",
    "        pil_image = pil_image.resize((width, height))\n",
    "\n",
    "    if output_type == \"torch\":\n",
    "        perturbed_image = transform(pil_image)\n",
    "    elif output_type == \"numpy\":\n",
    "        perturbed_image = np.array(pil_image)\n",
    "    elif output == \"pil\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Unsupported output type provided!\")\n",
    "\n",
    "    return(perturbed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:07<00:00, 66.50it/s]\n"
     ]
    }
   ],
   "source": [
    "image_names = []\n",
    "for image_np, _, image_name in tqdm(test_loader):\n",
    "    image_names.append(image_name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New loss term experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tiny batch containing 3 images; 2 random images from the test set, and a fully white image\n",
    "\n",
    "zeros_tensor = torch.zeros(1, 3, 640, 640)\n",
    "#zero_output = model(zeros_tensor)[0]\n",
    "\n",
    "random_index = random.randrange(len(image_names))\n",
    "other_image = perturb_img(image_names[random_index], change_aspect_ratio = False, output_type = \"torch\").unsqueeze(0)\n",
    "random_index = random.randrange(len(image_names))\n",
    "other_image_2 = perturb_img(image_names[random_index], change_aspect_ratio = False, output_type = \"torch\").unsqueeze(0)\n",
    "pil_image = transforms.ToPILImage()(other_image.squeeze())\n",
    "#display(pil_image)\n",
    "\n",
    "tiny_batch = torch.cat((other_image, other_image_2, zeros_tensor))\n",
    "#tiny_batch_output = model(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_yolos.yolov8.ultralytics.models.yolo.model import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 car, 1: 640x640 2 cars, 2: 640x640 (no detections), 914.2ms\n",
      "Speed: 0.0ms preprocess, 304.7ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "results = model(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6587e+02, 2.1483e+02, 4.0299e+02, 2.6229e+02, 3.3160e-01, 2.0000e+00],\n",
       "        [4.4693e+02, 2.2007e+02, 5.0874e+02, 2.7558e+02, 2.8920e-01, 2.0000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1].boxes.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attacks_tools.attack_utils import CustomPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "#ONDERSTAANDE OUTPUT MOET NAAR DEZE SHAPE WORDEN OMGEZET\n",
    "\n",
    "\"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n",
    "\n",
    "Returns:\n",
    "        list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "\"\"\"\n",
    "\n",
    "#output_patch.size() = torch.Size([8, 25200, 85])\n",
    "    #8 plaatjes per batch\n",
    "    #25200 bounding boxes per plaatje\n",
    "    #85 = (x, y, width, height), object confidence score, and class probabilities. (dus 80 class probs)\n",
    "#print(output_patch.size())\n",
    "\n",
    "predictor = CustomPredictor(overrides = dict(model='yolov8n.pt') )\n",
    "results = predictor(tiny_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 84, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][..., :4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8400, 84])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([     0.0030,     -0.0644,      9.5758,     36.7357,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
       "             0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
       "             0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
       "             0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.swapaxes(results[0], 1, 2).shape)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "torch.swapaxes(results[0], 1, 2)[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_yolos.yolov8.ultralytics.utils.ops import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_max_suppression(results[0])[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/coenschoof/miniconda/envs/phantomsponges/PhantomSponges'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.task_map[self.task][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<local_yolos.yolov8.ultralytics.models.yolo.detect.predict.DetectionPredictor at 0x7fad85896e50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.task_map['detect']['predictor']()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
